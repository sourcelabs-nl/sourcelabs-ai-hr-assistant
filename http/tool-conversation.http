POST http://localhost:11434/api/chat
Content-Type: application/json

// This prompting session returns the tool_calls in the response as expected.
// See the messages in the order they are defined.
// Interesting to note that LLMs are stateless. The tool calling interaction should
// be included into the messages send to the model. Where the LLM can trace back what happened
// with the requested tool call and provided result by the tool.

{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "What is the weather today in Paris?"
    },
    {
      "role": "assistant",
      "content": "",
      "tool_calls": [
        {
          "function": {
            "name": "get_current_weather",
            "arguments": {
              "format": "celsius",
              "location": "Paris"
            }
          }
        }
      ]
    },
    {
      "role": "tool",
      "content": "25 degrees celsius in Paris"
    }
  ],
  "stream": false,
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_weather",
        "description": "Get the current weather for a location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The location to get the weather for, e.g. San Francisco, CA"
            },
            "format": {
              "type": "string",
              "description": "The format to return the weather in, e.g. 'celsius' or 'fahrenheit'",
              "enum": [
                "celsius",
                "fahrenheit"
              ]
            }
          },
          "required": [
            "location",
            "format"
          ]
        }
      }
    }
  ]
}